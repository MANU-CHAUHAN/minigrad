# minigrad
## A small autograd repo implementing backpropagation with ability to develop small Neural Networks.

### Greatly inspired by Andrej Karpathy's  [micrograd](https://github.com/karpathy/micrograd)

----------

#### TODO:

1) utils and visualizations

2) Cover all operations (ones highlighted are done):
```
`Relu`, Log, Exp                        # unary ops
Sum, Max                                # reduce ops (with axis argument)
`Add`, `Sub`, `Mul`, `Pow`              # binary ops (with broadcasting)
Reshape, Transpose, Slice               # movement ops
Matmul, Conv2D                          # processing ops
```




### Reverse Mode Auto DIfferentiation

Let's say we have expression ğ‘§=ğ‘¥1ğ‘¥2+sin(ğ‘¥1) and want to find derivatives ğ‘‘ğ‘§ğ‘‘ğ‘¥1 and ğ‘‘ğ‘§ğ‘‘ğ‘¥2. Reverse-mode AD splits this task into 2 parts, namely, forward and reverse passes.

#### Forward pass:

First step is to decompose the complex expression into a set of primitive ones, i.e. expressions consisting of at most single step or single function call.
$$
ğ‘¤1=ğ‘¥1\\

ğ‘¤2=ğ‘¥2\\

ğ‘¤3=ğ‘¤1ğ‘¤2\\

ğ‘¤4=sin(ğ‘¤1)\\

ğ‘¤5=ğ‘¤3+ğ‘¤4\\

ğ‘§=ğ‘¤5\\
$$


The advantage of this representation is that differentiation rules for each separate expression are already known.

For example, we know that derivative of `sin` is `cos`, and so `dw4/dw1=cosâ¡(w1)`.

We will use this fact in reverse pass below. Essentially, forward pass consists of evaluating each of these expressions and saving the results. 

Say, our inputs are: `ğ‘¥1=2`  and `ğ‘¥2=3`. Then we have:
$$
ğ‘¤1=ğ‘¥1=2\\

ğ‘¤2=ğ‘¥2=3\\

ğ‘¤3=ğ‘¤1ğ‘¤2=6\\

ğ‘¤4=sin(ğ‘¤1) =0.9\\

ğ‘¤5=ğ‘¤3+ğ‘¤4=6.9\\

ğ‘§=ğ‘¤5=6.9
$$




#### Reverse pass:

This is the main part and it uses the **chain rule**.

In its basic form, chain rule states that if you have variable `ğ‘¡(ğ‘¢(ğ‘£))` which depends on `ğ‘¢` which, in its turn, depends on `ğ‘£`, then:
$$
\\
ğ‘‘ğ‘¡/ğ‘‘ğ‘£ = ğ‘‘ğ‘¡/ğ‘‘ğ‘¢ * ğ‘‘ğ‘¢/ğ‘‘ğ‘£\\
$$
or, if `ğ‘¡` depends on `ğ‘£` via several paths / variables `ğ‘¢ğ‘–`, e.g.:
$$
ğ‘¢1=ğ‘“(ğ‘£)\\

ğ‘¢2=ğ‘”(ğ‘£)\\

ğ‘¡=â„(ğ‘¢1,ğ‘¢2)\\
then:\\
ğ‘‘ğ‘¡/ğ‘‘ğ‘£=\sum_{i} ğ‘‘ğ‘¡/ğ‘‘ğ‘¢ğ‘– *ğ‘‘ğ‘¢ğ‘–/ğ‘‘ğ‘£
$$
In terms of expression graph, if we have a final node `ğ‘§` and input nodes `ğ‘¤ğ‘–`, and path from `ğ‘§` to `ğ‘¤ğ‘–` goes through intermediate nodes `ğ‘¤ğ‘` (i.e. ğ‘§=ğ‘”(ğ‘¤ğ‘) where ğ‘¤ğ‘=ğ‘“(ğ‘¤ğ‘–)), we can find derivative `ğ‘‘ğ‘§/ğ‘‘ğ‘¤ğ‘–` as


$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤ğ‘–=\sum_{ğ‘âˆˆPğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ (ğ‘–)}ğ‘‘ğ‘§/ğ‘‘ğ‘¤ğ‘ * ğ‘‘ğ‘¤ğ‘/ğ‘‘ğ‘¤ğ‘–
$$
In other words, to calculate the derivative of output variable ğ‘§ w.r.t. any intermediate or input variable ğ‘¤ğ‘–, we only need to know the derivatives of its parents and the formula to calculate derivative of primitive expression ğ‘¤ğ‘=ğ‘“(ğ‘¤ğ‘–).



Reverse pass starts at the end (i.e. ğ‘‘ğ‘§/ğ‘‘ğ‘§) and propagates backward to all dependencies. 
$$
ğ‘‘ğ‘§/ğ‘‘ğ‘§=1\\
$$
Then we know that ğ‘§=ğ‘¤5 and so:

â€‹                                                                                                         ğ‘‘ğ‘§/ğ‘‘ğ‘¤5 = 1

ğ‘¤5 linearly depends on ğ‘¤3 and ğ‘¤4, so ğ‘‘ğ‘¤5/ğ‘‘ğ‘¤3=1 and ğ‘‘ğ‘¤5/ğ‘‘ğ‘¤4=1. Using the chain rule we find:
$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤3=ğ‘‘ğ‘§/ğ‘‘ğ‘¤5 Ã— ğ‘‘ğ‘¤5/ ğ‘‘ğ‘¤3 = 1Ã—1 = 1\\
 
ğ‘‘ğ‘§/ğ‘‘ğ‘¤4=ğ‘‘ğ‘§/ğ‘‘ğ‘¤5 Ã— ğ‘‘ğ‘¤5/ğ‘‘ğ‘¤4 = 1Ã—1 = 1
$$


From definition ğ‘¤3=ğ‘¤1ğ‘¤2 and rules of partial derivatives, we find that ğ‘‘ğ‘¤3 / ğ‘‘ğ‘¤2=ğ‘¤1. Thus:
$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤2 = ğ‘‘ğ‘§/ğ‘‘ğ‘¤3 Ã— ğ‘‘ğ‘¤3/ğ‘‘ğ‘¤2 = 1 Ã— ğ‘¤1 = ğ‘¤1
$$
Which, as we already know from forward pass, is:
$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤2 = ğ‘¤1 = 2
$$


Finally, `ğ‘¤1` contributes to `ğ‘§` via `ğ‘¤3` and `ğ‘¤4`. Once again, from the rules of partial derivatives we know that `ğ‘‘ğ‘¤3/ğ‘‘ğ‘¤1 = ğ‘¤2` and `ğ‘‘ğ‘¤4/ğ‘‘ğ‘¤1 = cos(ğ‘¤1)`. Thus:
$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤1 = ğ‘‘ğ‘§/ğ‘‘ğ‘¤3 * ğ‘‘ğ‘¤3/ğ‘‘ğ‘¤1 + ğ‘‘ğ‘§/ğ‘‘ğ‘¤4 * ğ‘‘ğ‘¤4/ğ‘‘ğ‘¤1 = ğ‘¤2 + cos(ğ‘¤1)
$$


And again, given known inputs, we can calculate it:


$$
ğ‘‘ğ‘§/ğ‘‘ğ‘¤1 = ğ‘¤2 + cos(ğ‘¤1) = 3 + cos(2) = 2.58
$$


Since ğ‘¤1 and ğ‘¤2 are just aliases for ğ‘¥1 and ğ‘¥2, we get our answer:
$$
ğ‘‘ğ‘§ / ğ‘‘ğ‘¥1 = 2.58\\

 
ğ‘‘ğ‘§ / ğ‘‘ğ‘¥2 = 2
$$
